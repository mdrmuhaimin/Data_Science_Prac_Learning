{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_resolution.py\n",
    "import re\n",
    "import operator\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, types, functions\n",
    "from pyspark.sql.functions import concat, concat_ws, col, lit, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('entity_res').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityResolution:\n",
    "    def __init__(self, dataFile1, dataFile2, stopWordsFile):\n",
    "        self.f = open(stopWordsFile, \"r\")\n",
    "        self.stopWords = set(self.f.read().split(\"\\n\"))\n",
    "        self.stopWordsBC = sc.broadcast(self.stopWords).value\n",
    "        self.df1 = spark.read.parquet(dataFile1).cache()\n",
    "        self.df2 = spark.read.parquet(dataFile2).cache()\n",
    "\n",
    "        \n",
    "        \n",
    "    def preprocessDF(self, df, cols): \n",
    "        \"\"\" \n",
    "            Input: $df represents a DataFrame\n",
    "                   $cols represents the list of columns (in $df) that will be concatenated and be tokenized\n",
    "\n",
    "            Output: Return a new DataFrame that adds the \"joinKey\" column into the input $df\n",
    "\n",
    "            Comments: The \"joinKey\" column is a list of tokens, which is generated as follows:\n",
    "                     (1) concatenate the $cols in $df; \n",
    "                     (2) apply the tokenizer to the concatenated string\n",
    "            Here is how the tokenizer should work:\n",
    "                     (1) Use \"re.split(r'\\W+', string)\" to split a string into a set of tokens\n",
    "                     (2) Convert each token to its lower-case\n",
    "                     (3) Remove stop words\n",
    "        \"\"\"\n",
    "        stop_words = self.stopWordsBC\n",
    "        def tokenized_filterized_string (string):\n",
    "            string = re.sub('\\s+',' ',string).strip().lower() # Remove extra whitespace and finally remove trailing spaces\n",
    "            tokens = re.split(r'\\W+', string)\n",
    "            stop_words.add('')\n",
    "            tokens = set(tokens) - stop_words\n",
    "            return list(tokens)\n",
    "        \n",
    "        get_tokenized_string = functions.udf(tokenized_filterized_string, types.ArrayType(types.StringType()))\n",
    "        concatanated_column = 'joinKey'\n",
    "        df = df.withColumn(concatanated_column, concat_ws(' ', df[cols[0]], df[cols[1]]))\n",
    "        df = df.withColumn(concatanated_column, get_tokenized_string(df[concatanated_column]))                           \n",
    "        return df\n",
    "\n",
    "    def filtering(self, df1, df2):\n",
    "        \"\"\" \n",
    "            Input: $df1 and $df2 are two input DataFrames, where each of them \n",
    "                   has a 'joinKey' column added by the preprocessDF function\n",
    "\n",
    "            Output: Return a new DataFrame $candDF with four columns: 'id1', 'joinKey1', 'id2', 'joinKey2',\n",
    "                    where 'id1' and 'joinKey1' are from $df1, and 'id2' and 'joinKey2'are from $df2.\n",
    "                    Intuitively, $candDF is the joined result between $df1 and $df2 on the condition that \n",
    "                    their joinKeys share at least one token. \n",
    "\n",
    "            Comments: Since the goal of the \"filtering\" function is to avoid n^2 pair comparisons, \n",
    "                      you are NOT allowed to compute a cartesian join between $df1 and $df2 in the function. \n",
    "                      Please come up with a more efficient algorithm (see hints in Lecture 2). \n",
    "        \"\"\"\n",
    "        \n",
    "        df1 = df1.select('id', 'joinKey').withColumn(\"flattened_key\", explode(df1['joinKey']))\n",
    "        df2 = df2.select('id', 'joinKey').withColumn(\"flattened_key\", explode(df2['joinKey']))\n",
    "        df1.createOrReplaceTempView(\"df1\")\n",
    "        df2.createOrReplaceTempView(\"df2\")\n",
    "        common_item = spark.sql('SELECT distinct df1.id as id1, df1.joinKey as joinKey1, df2.id as id2, df2.joinKey as joinKey2 \\\n",
    "        FROM df1, df2 WHERE df1.flattened_key = df2.flattened_key')\n",
    "#         common_items = df1.select('flattened_key').distinct().intersect(df2.select('flattened_key')).collect()\n",
    "#         print(common_items)\n",
    "        return common_item\n",
    "        \n",
    "        \n",
    "\n",
    "    def verification(self, candDF, threshold):\n",
    "        \"\"\" \n",
    "            Input: $candDF is the output DataFrame from the 'filtering' function. \n",
    "                   $threshold is a float value between (0, 1] \n",
    "\n",
    "            Output: Return a new DataFrame $resultDF that represents the ER result. \n",
    "                    It has five columns: id1, joinKey1, id2, joinKey2, jaccard \n",
    "\n",
    "            Comments: There are two differences between $candDF and $resultDF\n",
    "                      (1) $resultDF adds a new column, called jaccard, which stores the jaccard similarity \n",
    "                          between $joinKey1 and $joinKey2\n",
    "                      (2) $resultDF removes the rows whose jaccard similarity is smaller than $threshold \n",
    "        \"\"\"\n",
    "        def get_jaccard_similarity(set_1, set_2):\n",
    "            set_1 = set(set_1)\n",
    "            set_2 = set(set_2)\n",
    "            return len(set_1 & set_2) * 1.00 / len(set_1 | set_2) * 1.00\n",
    "            \n",
    "        calculate_jaccard = functions.udf(get_jaccard_similarity, types.DoubleType())\n",
    "        candDF = candDF.withColumn('jaccard', calculate_jaccard(candDF['joinKey1'], candDF['joinKey2']))\n",
    "        candDF = candDF.filter(candDF.jaccard >= threshold)\n",
    "        return candDF\n",
    "    \n",
    "\n",
    "    def evaluate(self, result, groundTruth):\n",
    "        \"\"\"\n",
    "            Input: $result is a list of matching pairs identified by the ER algorithm\n",
    "                   $groundTrueth is a list of matching pairs labeld by humans\n",
    "\n",
    "            Output: Compute precision, recall, and fmeasure of $result based on $groundTruth, and\n",
    "                    return the evaluation result as a triple: (precision, recall, fmeasure)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        result_count = len(result) # Value of R\n",
    "        groundTruth_count = len(groundTruth) # Value of A\n",
    "        correctly_identified_result = set(result) & set(groundTruth)\n",
    "        correctly_identified_result_count = len(correctly_identified_result) # Value of T\n",
    "        precision = correctly_identified_result_count / result_count\n",
    "        recall = correctly_identified_result_count / groundTruth_count\n",
    "        fm_measure = 2 * precision * recall / (precision + recall)\n",
    "        return (precision, recall, fm_measure)\n",
    "\n",
    "    def jaccardJoin(self, cols1, cols2, threshold):\n",
    "        newDF1 = self.preprocessDF(self.df1, cols1)\n",
    "        newDF2 = self.preprocessDF(self.df2, cols2)\n",
    "        print (\"Before filtering: %d pairs in total\" %(self.df1.count()*self.df2.count())) \n",
    "        candDF = self.filtering(newDF1, newDF2)\n",
    "        print (\"After Filtering: %d pairs left\" %(candDF.count()))\n",
    "\n",
    "        resultDF = self.verification(candDF, threshold)\n",
    "        print (\"After Verification: %d similar pairs\" %(resultDF.count()))\n",
    "\n",
    "        return resultDF\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 4397038 pairs in total\n",
      "After Filtering: 689922 pairs left\n",
      "After Verification: 2030 similar pairs\n",
      "(precision, recall, fmeasure) =  (0.3699507389162562, 0.5776923076923077, 0.4510510510510511)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    er = EntityResolution(\"Amazon\", \"Google\", \"stopwords.txt\")\n",
    "    amazonCols = [\"title\", \"manufacturer\"]\n",
    "    googleCols = [\"name\", \"manufacturer\"]\n",
    "    resultDF = er.jaccardJoin(amazonCols, googleCols, 0.5)\n",
    "    \n",
    "    result = resultDF.rdd.map(lambda row: (row.id1, row.id2)).collect()\n",
    "    groundTruth = spark.read.parquet(\"Amazon_Google_perfectMapping\").rdd \\\n",
    "                          .map(lambda row: (row.idAmazon, row.idGoogle)).collect()\n",
    "    print (\"(precision, recall, fmeasure) = \", er.evaluate(result, groundTruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
