{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_resolution.py\n",
    "import re\n",
    "import operator\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, types, functions\n",
    "from pyspark.sql.functions import concat, concat_ws, col, lit, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('entity_res').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityResolution:\n",
    "    def __init__(self, dataFile1, dataFile2, stopWordsFile):\n",
    "        self.f = open(stopWordsFile, \"r\")\n",
    "        self.stopWords = set(self.f.read().split(\"\\n\"))\n",
    "        self.stopWordsBC = sc.broadcast(self.stopWords).value\n",
    "        self.df1 = spark.read.parquet(dataFile1).cache()\n",
    "        self.df2 = spark.read.parquet(dataFile2).cache()\n",
    "\n",
    "        \n",
    "        \n",
    "    def preprocessDF(self, df, cols): \n",
    "        stop_words = self.stopWordsBC\n",
    "        def tokenized_filterized_string (string):\n",
    "            string = re.sub('[^0-9a-zA-Z]+', ' ', string) # Remove any non alpha-numeric charecter with whitespace\n",
    "            string = re.sub('\\s+',' ',string).strip().lower() # Remove extra whitespace and finally remove trailing spaces\n",
    "            tokens = re.split(r'\\W+', string)\n",
    "            tokens = set(tokens) - stop_words\n",
    "            return list(tokens)\n",
    "        \n",
    "        get_tokenized_string = functions.udf(tokenized_filterized_string, types.ArrayType(types.StringType()))\n",
    "        concatanated_column = 'joinKey'\n",
    "        df = df.withColumn(concatanated_column, concat_ws(' ', df[cols[0]], df[cols[1]]))\n",
    "        df = df.withColumn(concatanated_column, get_tokenized_string(df[concatanated_column]))                           \n",
    "        return df\n",
    "\n",
    "    def filtering(self, df1, df2):\n",
    "        df1 = df1.withColumn(\"flattened_key\", explode(df1['joinKey']))\n",
    "        df2 = df2.withColumn(\"flattened_key\", explode(df2['joinKey']))\n",
    "        df1.createOrReplaceTempView(\"df1\")\n",
    "        df2.createOrReplaceTempView(\"df2\")\n",
    "        common_item = spark.sql('SELECT df1.id as id1, df1.joinKey as joinKey1, df2.id as id2, df2.joinKey as joinKey2 \\\n",
    "        FROM df1, df2 WHERE df1.flattened_key = df2.flattened_key')\n",
    "#         common_items = df1.select('flattened_key').distinct().intersect(df2.select('flattened_key')).collect()\n",
    "#         print(common_items)\n",
    "        return common_item\n",
    "        \n",
    "        \n",
    "\n",
    "    def verification(self, candDF, threshold):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "        def get_jaccard_similarity(set_1, set_2):\n",
    "            set_1 = set(set_1)\n",
    "            set_2 = set(set_2)\n",
    "            return len(list(set_1 & set_2)) / len(list(set_1 | set_2))\n",
    "            \n",
    "        calculate_jaccard = functions.udf(get_jaccard_similarity, types.DoubleType())\n",
    "        candDF = candDF.withColumn('jaccard', calculate_jaccard(candDF['joinKey1'], candDF['joinKey2']))\n",
    "        candDF = candDF.filter(candDF.jaccard >= threshold)\n",
    "        return candDF\n",
    "\n",
    "    def evaluate(self, result, groundTruth):\n",
    "        \"\"\"\n",
    "            Write your code!\n",
    "        \"\"\"\n",
    "\n",
    "    def jaccardJoin(self, cols1, cols2, threshold):\n",
    "        newDF1 = self.preprocessDF(self.df1, cols1)\n",
    "        newDF2 = self.preprocessDF(self.df2, cols2)\n",
    "        print (\"Before filtering: %d pairs in total\" %(self.df1.count()*self.df2.count())) \n",
    "        candDF = self.filtering(newDF1, newDF2)\n",
    "        print (\"After Filtering: %d pairs left\" %(candDF.count()))\n",
    "\n",
    "        resultDF = self.verification(candDF, threshold)\n",
    "        resultDF.show()\n",
    "#         print (\"After Verification: %d similar pairs\" %(resultDF.count()))\n",
    "\n",
    "#         return resultDF\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 256 pairs in total\n",
      "After Filtering: 145 pairs left\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-208cd52b1d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mamazonCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"manufacturer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgoogleCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"manufacturer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mresultDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaccardJoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazonCols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoogleCols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     result = resultDF.map(lambda row: (row.id1, row.id2)).collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-e398999dc25c>\u001b[0m in \u001b[0;36mjaccardJoin\u001b[0;34m(self, cols1, cols2, threshold)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"After Filtering: %d pairs left\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mresultDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mresultDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#         print (\"After Verification: %d similar pairs\" %(resultDF.count()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-e398999dc25c>\u001b[0m in \u001b[0;36mverification\u001b[0;34m(self, candDF, threshold)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mcalculate_jaccard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_jaccard_similarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mcandDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jaccard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_jaccard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'joinKey1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'joinKey2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mcandDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaccard\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcandDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    er = EntityResolution(\"Amazon_sample\", \"Google_sample\", \"stopwords.txt\")\n",
    "    amazonCols = [\"title\", \"manufacturer\"]\n",
    "    googleCols = [\"name\", \"manufacturer\"]\n",
    "    resultDF = er.jaccardJoin(amazonCols, googleCols, 0.5)\n",
    "\n",
    "#     result = resultDF.map(lambda row: (row.id1, row.id2)).collect()\n",
    "#     groundTruth = spark.read.parquet(\"Amazon_Google_perfectMapping_sample\") \\\n",
    "#                           .map(lambda row: (row.idAmazon, row.idGoogle)).collect()\n",
    "#     print (\"(precision, recall, fmeasure) = \", er.evaluate(result, groundTruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
