{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists often need to crawl data from websites and turn the crawled data (HTML pages) to structured data (tables). Thus, web scraping is an essential skill that every data scientist should master. In this assignment, you will learn the followings:\n",
    "\n",
    "\n",
    "* How to use [requests](http://www.python-requests.org/en/master/) to download HTML pages from a website?\n",
    "* How to select content on a webpage with [lxml](http://lxml.de/)? \n",
    "\n",
    "You can either use Spark DataFrame or [pandas.DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) to do the assignment. In comparison, pandas.DataFrame has richer APIs, but is not good at distributed computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time to write a web scraper, you need to learn some basic knowledge of HTML, DOM, and XPath. I found that this is a good resource: [https://data-lessons.github.io/library-webscraping/](https://data-lessons.github.io/library-webscraping/). Please take a look at\n",
    "\n",
    "* [Selecting content on a web page with XPath\n",
    "](https://data-lessons.github.io/library-webscraping/xpath/)\n",
    "* [Web scraping using Python: requests and lxml](https://data-lessons.github.io/library-webscraping/04-lxml/). \n",
    "\n",
    "Please let me know if you find a better resource. I'll share it with the other students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are a data scientist working at SFU. One day, you want to analyze CS faculty data and answer two interesting questions:\n",
    "\n",
    "1. Who are the CS faculty members?\n",
    "2. What are their research interests?\n",
    "\n",
    "To do so, the first thing is to figure out what data to collect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: SFU CS Faculty Members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You find that there is a web page in the CS school website, which lists all the faculty members as well as their basic information. \n",
    "\n",
    "In Task 1, your job is to write a web scraper to extract the faculty information from this page: [https://www.sfu.ca/computing/people/faculty.html](https://www.sfu.ca/computing/people/faculty.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Crawling Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A web page is essentially a file stored in a remote machine (called web server). You can use [requests](http://www.python-requests.org/en/master/) to open such a file and read data from it. Please complete the following code to download the HTML page and save it as a text file (like [this](./faculty.txt)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Download the webpage\n",
    "domain = 'http://www.sfu.ca'\n",
    "r = requests.get('https://www.sfu.ca/computing/people/faculty.html')\n",
    "\n",
    "# 2. Save it as a text file (named faculty.txt)\n",
    "web_html = r.text\n",
    "f = open('faculty.txt','w')\n",
    "f.write(web_html)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Extracting Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An HTML page follows the Document Object Model (DOM). It models an HTML page as a tree structure wherein each node is an object representing a part of the page. The nodes can be searched and extracted programmatically using XPath. Please complete the following code to transform the above HTML page to a CSV file (like [this](./faculty_table.csv)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml\n",
    "import lxml.html\n",
    "import cssselect\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Open faculty.txt\n",
    "faculty_text = open('faculty.txt', 'r').read()\n",
    "\n",
    "# 2. Parse the HTML page as a tree structure\n",
    "parsed_html = lxml.etree.HTML(faculty_text)\n",
    "\n",
    "# 3. Extract related content from the tree using XPath\n",
    "faculty_list = parsed_html.xpath(\"//*[contains(concat(' ', @class, ' '), ' faculty-list ')]\")[0].xpath(\".//*[contains(concat(' ', @class,' '),' textimage section ')]/div/*[contains(concat(' ', @class,' '),' text ')]\")\n",
    "info_list = list()\n",
    "for fac in faculty_list:\n",
    "    fac_name_pos = fac.cssselect(\"h4\")[0].text.split(', ', 1 )\n",
    "    fac_name = fac_name_pos[0].strip()\n",
    "    fac_pos = fac_name_pos[1].strip()\n",
    "    area_ins_component = fac.cssselect(\"p\")[0]\n",
    "    area_ins = lxml.etree.tostring(area_ins_component, encoding='UTF-8', method=\"text\").decode('UTF-8') #.replace(\"Area: \", \"\", 1)\n",
    "    area_ins = re.sub('Area:\\s+', '', area_ins)\n",
    "    area_ins = re.sub('\\s+',' ',area_ins).strip() # Remove extra whitespace and finally remove trailing spaces\n",
    "    contact_homepage = fac.cssselect(\"p a\")\n",
    "    profile = fac.xpath(\"./p/a[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'profile & contact information')]\")[0].get(\"href\")\n",
    "\n",
    "    if not profile.startswith( 'http' ):\n",
    "        profile = \"{}{}\".format(domain, profile).strip()\n",
    "    homepage = \"\"\n",
    "    homepage_xpathelem = fac.xpath(\"./p/a[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'home page')]\")\n",
    "\n",
    "    if len(homepage_xpathelem) > 0:\n",
    "        homepage = homepage_xpathelem[0].get(\"href\")\n",
    "        if not homepage.startswith( 'http' ):\n",
    "            homepage = \"{}{}\".format(domain, homepage)\n",
    "        homepage = homepage.strip()\n",
    "\n",
    "    info_list.append((fac_name, fac_pos, area_ins, profile, homepage))    \n",
    "\n",
    "# 4. Save the extracted content as an csv file (named faculty_table.csv)\n",
    "labels = ['name', 'rank', 'area', 'profile', 'homepage']\n",
    "faculty_table = pd.DataFrame(info_list, columns=labels)\n",
    "faculty_table.to_csv('faculty_table.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Research Interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to know the research interests of each faculty. However, the above crawled web page does not contain such information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Crawling Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice that such information can be found on the profile page of each faculty. For example, you can find the research interests of Dr. Jiannan Wang from [http://www.sfu.ca/computing/people/faculty/jiannanwang.html](http://www.sfu.ca/computing/people/faculty/jiannanwang.html). \n",
    "\n",
    "\n",
    "Please complete the following code to download the profile pages and save them as text files. There are 56 faculties, so you need to download 56 web pages in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for idx, profile_url in enumerate(faculty_table['profile']):\n",
    "    name = faculty_table['name'][idx]\n",
    "\n",
    "    # 1. Download the profile pages of 56 faculties\n",
    "    r = requests.get(profile_url)\n",
    "\n",
    "    # 2. Save each page as a text file\n",
    "    web_html = r.text\n",
    "    f = open('{}.txt'.format(name),'w')\n",
    "    f.write(web_html)\n",
    "    f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Extracting Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the following code to extract the research interests of each faculty, and generate a file like [this](./faculty_more_table.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greg Baker []\n",
      "Petra Berenbrink [Probabilistic methods, Randomized algorithms, Analysis of dynamic processes, Load balancing and resource allocation, Communication in distributed systems]\n",
      "Andrei Bulatov [Constraint problems, Computational complexity, Combinatorics, Clone theory and universal algebra]\n",
      "BOBBY CHAN []\n",
      "LEONID CHINDELEVITCH [Bioinformatics, Computational Biology, Computational Epidemiology of Infectious Diseases]\n",
      "Diana Cukierman [Computing Science Education, Learning strategies tailored to CS and STEM disciplines (see Academic Enhancement Program), Pedagogy, active learning, communication skills, Artificial Intelligence]\n",
      "James P. Delgrande [Knowledge representation and reasoning, Belief change, Reasoning about action and change, Nonmonotonic reasoning, Preference handling]\n",
      "Toby Donaldson []\n",
      "John Edgar []\n",
      "Martin Ester [Social network analysis, Recommender systems, Opinion mining and information extraction, Biological network analysis, Next-gen sequencing]\n",
      "Brian Fraser []\n",
      "Yasutaka Furukawa [Computer Vision, Deep Learning, Computer Graphics, Computational Photography]\n",
      "Uwe Glässer [Big data intelligence, Situation analysis and decision support, Criminal network analysis, Cybersecurity]\n",
      "Mohamed Hefeeda [Multimedia networking over wired and wireless networks, Peer-to peer systems, Internet Protocols, Cloud and high-performance computing]\n",
      "Harinder Singh Khangura []\n",
      "Anne Lavergne []\n",
      "Maxwell Libbrecht [machine learning, probabilistic modeling, unsupervised learning, submodular optimization, genomics, gene regulation]\n",
      "Jiangchuan (JC) Liu [Internet architecture and protocols, Multimedia content, distribution, and processing, Wireless mobile networking, Cloud computing and big data networking, Online gaming and social networking]\n",
      "David Mitchell [Propositional satisfiability and finite domain constraint satisfaction results on the complexity of these problems aspects of design of practical algorithms for SAT and CSP application to formal verification and related tasks]\n",
      "Jian Pei [Data mining, Data warehousing and online analytic processing (OLAP), Database systems, Bioinformatics, Data mining and database systems, Business intelligence, data warehousing and online analytic processing (OLAP), Web search and information retrieval, Social media and social networks, Health and medical informatics]\n",
      "Fred Popowich [How computers can be used to process human language either to make it easier for human beings to interact with computers, or to make it easier for human beings to interact with each other., Intelligent Systems and their Applications]\n",
      "Arrvindh Shriraman [Improving energy efficiency of software, Multicore memory systems, Hardware/Software Interface for improving programming, Synchronization]\n",
      "William (Nick) Sumner [Automated debugging & debugging tools, Concurrency & parallelism, Program analysis & transformation]\n",
      "Ping Tan [Computer Vision, Computer Graphics, Robotics]\n",
      "Eugenia Ternovska [Logic-based artificial intelligence, Knowledge representation, Efficient reasoning]\n",
      "Keval Vora [Parallel & Distributed Computing, Irregular Big Data Processing, High Performance Computing]\n",
      "Ke Wang [Mining massive datasets, Mining biological data, Graph and network data, Data privacy]\n",
      "Kay C. Wiese [Bioinformatics and Computational Biology, Evolutionary Computation, Computational Intelligence and Machine Learning]\n",
      "KangKang Yin [computer animation, computer graphics, humanoid robotics, machine learning, multimedia analysis]\n",
      "Brad Bart []\n",
      "Binay Bhattacharya [Design and analysis of geometric algorithms, Approximation algorithms, Resource allocation optimization, Vehicle routing, Transportation scheduling]\n",
      "Robert D. Cameron [High performance parsing and text processing, SIMD and multicore parallelism, Programming language technology, Software systems, Open software technology]\n",
      "Mo Chen [Multi-robot systems, Safety-critical systems, Motion planning, Control theory, Computational methods]\n",
      "Parmit Chilana [human-computer interaction, design of interactive systems, crowdsourcing/ social computing]\n",
      "Ryan C.N. D'Arcy []\n",
      "Mark S. Drew [Image processing, colour, computer vision, computer graphics, multimedia, and visualization.]\n",
      "Brian Funt [Digital Colour Imaging, Colour Constancy, Colour in Medicine, Colour Perception]\n",
      "Qianping Gu [Network communications, Parallel/distributed computing, Algorithms and computation, Machine learning, Computational biology]\n",
      "Ghassan Hamarneh [Medical Image Analysis]\n",
      "Eugene Fiume [Computer graphics]\n",
      "Pavol Hell [Algorithmic graph theory, Complexity of algorithms, Combinatorics of networks]\n",
      "Valentine Kabanets [Computational complexity theory, Pseudorandomness and derandomization, Circuit lower bounds, Logic in complexity theory]\n",
      "Ramesh Krishnamurti [Linear programming, Duality, Matching, Designing Approximation algorithms based on linear programming]\n",
      "Ze-Nian Li [Computer vision, Multimedia, Artificial Intelligence]\n",
      "Angelica Lim [Human Robot Interaction, Affective Computing, Multimodal Perception and Learning, Developmental Robotics]\n",
      "Greg Mori [Computer Vision, Machine Learning, Video Analysis, Human Activity Recognition, Object Recognition]\n",
      "Steven Pearce [Spectral Methods in Mathematics, Computational Magnetohydrodynamics applied to Astrophysical Dynamo Theory, Computational Fluid Dynamics applied to Cratering Mechanics and Oceanic Asteroidal Impact Generation of Megatsunamis, Mathematical Inverse Theory applied to High Energy Astrophysics, The Theory of Technology, Graph Theoretic Approach to Cellular Automata]\n",
      "Joseph G. Peters [Modelling and performance analysis of communication networks, Communication algorithms, Distributed computation, Multimedia networking]\n",
      "Janice Regan []\n",
      "Anoop Sarkar [My research is focused on machine learning algorithms applied to natural language processing: in the areas of statistical machine translation, statistical parsing and formal language theory]\n",
      "Thomas C. Shermer [Software engineering: tools and methods, Compiler construction, Program transformations, Theoretical computer networks, Computational geometry]\n",
      "Oliver Schulte [Machine Learning, Statistical Learning for Relational Databases, Computational Game Theory, Computational Logic]\n",
      "Richard Vaughan [Applying models of animal behavior to solve problems in robotics and other adaptive systems, Creating tools and techniques for designing and simulating populations of robots]\n",
      "Jiannan Wang [Database Systems, Data Management, Data Cleaning, Crowdsourcing]\n",
      "Cynthia Xie []\n",
      "Richard (Hao) Zhang [Geometry processing, Computer graphics, Shape modeling and analysis]\n"
     ]
    }
   ],
   "source": [
    "import lxml\n",
    "import lxml.html\n",
    "import cssselect\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "research_interests_column = list()\n",
    "for name in faculty_table['name']:\n",
    "    # 1. Open each text file and parse it as a tree structure \n",
    "    profile_text = open('{}.txt'.format(name), 'r').read()\n",
    "    profile_text_tree = lxml.etree.HTML(profile_text)\n",
    "\n",
    "    # 2. Extract the research interests from each tree using XPath\n",
    "    research_interest = profile_text_tree.xpath(\"//div[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'research interests') and ul]\")\n",
    "    interests_list = list()\n",
    "    for interest in research_interest:\n",
    "        interest_names = interest.xpath('.//ul/li')\n",
    "        if len(interest_names) == 0:\n",
    "            interests_list.append([])\n",
    "\n",
    "        for interest_name in interest_names:\n",
    "            interest_name_text = lxml.etree.tostring(interest_name, encoding='UTF-8', method=\"text\").decode('UTF-8')\n",
    "            interest_name_text = re.sub('\\s+',' ',interest_name_text).strip() # Remove extra whitespace and finally remove trailing spaces\n",
    "            interests_list.append(interest_name_text)\n",
    "    interests_list = '[{}]'.format(', '.join(interests_list))            \n",
    "    research_interests_column.append(interests_list)\n",
    "\n",
    "        \n",
    "# 3. Add the extracted content to faculty_table.csv\n",
    "# 4. Generate a new CSV file, named faculty_more_table.csv\n",
    "faculty_table['research_interests'] = research_interests_column\n",
    "faculty_table.to_csv('faculty_more_table.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code in this [notebook](A1.ipynb), and submit it to the CourSys activity [Assignment 1](https://courses.cs.sfu.ca/2018sp-cmpt-733-g1/+a1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
