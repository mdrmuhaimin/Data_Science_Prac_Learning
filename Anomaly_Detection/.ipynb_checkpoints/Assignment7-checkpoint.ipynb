{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly_detection.py\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "import operator\n",
    "\n",
    "appName = 'anomaly_detection'\n",
    "conf = SparkConf().setAppName(appName)\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AnomalyDetection():\n",
    "    def readToyData(self):\n",
    "        data = [(0, [\"http\", \"udt\", 0.4]), \\\n",
    "                (1, [\"http\", \"udf\", 0.5]), \\\n",
    "                (2, [\"http\", \"tcp\", 0.5]), \\\n",
    "                (3, [\"ftp\", \"icmp\", 0.1]), \\\n",
    "                (4, [\"http\", \"tcp\", 0.4])]\n",
    "        schema = [\"id\", \"rawFeatures\"]\n",
    "        self.rawDF = spark.createDataFrame(data, schema)\n",
    "\n",
    "    def readData(self, filename, is_reading_toy_data=False):\n",
    "        if not is_reading_toy_data:\n",
    "            self.rawDF = spark.read.parquet(filename).cache()\n",
    "        else:\n",
    "            self.readToyData()\n",
    "\n",
    "    def cat2Num(self, df, indices):\n",
    "        \"\"\" \n",
    "            Input: $df represents a DataFrame with two columns: \"id\" and \"rawFeatures\"\n",
    "                   $indices represents which dimensions in $rawFeatures are categorical features, \n",
    "                    e.g., indices = [0, 1] denotes that the first two dimensions are categorical features.\n",
    "        \n",
    "            Output: Return a new DataFrame that adds the \"features\" column into the input $df\n",
    "        \n",
    "            Comments: The difference between \"features\" and \"rawFeatures\" is that \n",
    "            the latter transforms all categorical features in the former into numerical features \n",
    "            using one-hot key representation\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        features = df\n",
    "        for i in indices:\n",
    "            extract_cat = udf(lambda x: x[i], StringType())\n",
    "            features = features.withColumn(\"cat_{}\".format(i), extract_cat(features['rawFeatures'])) \n",
    "        for i in indices:\n",
    "            distinct = list(features.select('cat_{}'.format(i)).distinct().toPandas()['cat_{}'.format(i)])\n",
    "            one_hot_encode = udf(lambda x: [float(x == value) for value in distinct], ArrayType(FloatType()))\n",
    "            features = features.withColumn('cat_he_{}'.format(i), \n",
    "                                           one_hot_encode(features['cat_{}'.format(i)]))\n",
    "            features = features.drop('cat_{}'.format(i))\n",
    "            \n",
    "        def insert_feature_udf(raw_feature, new_element, index):\n",
    "            raw_feature[index] = ', '.join(str(i) for i in new_element)\n",
    "            return raw_feature\n",
    "        features = features.withColumn('features', features['rawFeatures'])\n",
    "        for i in indices:\n",
    "            insert_feature = udf(lambda x, y: insert_feature_udf(x, y, i), ArrayType(StringType()))\n",
    "            features = features.withColumn( 'features', \n",
    "                                        insert_feature(features['features'],  features['cat_he_{}'.format(i)]) ) \n",
    "            features = features.drop('cat_he_{}'.format(i))\n",
    "        to_float_list = udf(lambda x: [float(i) for i in (', '.join(x).replace(' ', '')).split(',')], ArrayType(DoubleType()))\n",
    "        features = features.withColumn('features', to_float_list(features['features']))\n",
    "        return features\n",
    "\n",
    "    def addScore(self, df):\n",
    "        \"\"\" \n",
    "            Input: $df represents a DataFrame with four columns: \"id\", \"rawFeatures\", \"features\", and \"prediction\"\n",
    "            Output: Return a new DataFrame that adds the \"score\" column into the input $df\n",
    "\n",
    "            To compute the score of a data point x, we use:\n",
    "\n",
    "                 score(x) = (N_max - N_x)/(N_max - N_min), \n",
    "\n",
    "            where N_max and N_min represent the size of the largest and smallest clusters, respectively,\n",
    "                  and N_x represents the size of the cluster assigned to x \n",
    "        \"\"\"\n",
    "        prediction_groupby = df.groupBy('prediction')\n",
    "        df_predcount = prediction_groupby.count().cache()\n",
    "        N_max = df_predcount.agg({'count': 'max'}).collect()[0]['max(count)']\n",
    "        N_min = df_predcount.agg({'count': 'min'}).collect()[0]['min(count)']\n",
    "        get_score = udf(lambda N_x: 0.0 if N_max == N_min else ((N_max - N_x)/(N_max - N_min)), FloatType())\n",
    "        df = df.join(df_predcount, 'prediction', 'inner').select('rawFeatures', 'features', 'prediction', 'count')\n",
    "        df = df.withColumn('score', get_score(df['count']))\n",
    "        return df.select('rawFeatures', 'features', 'prediction', 'score')\n",
    "\n",
    "    def detect(self, k, t):\n",
    "        #Encoding categorical features using one-hot.\n",
    "        df1 = self.cat2Num(self.rawDF, [0, 1]).cache()\n",
    "        df1.show()\n",
    "\n",
    "        #Clustering points using KMeans\n",
    "        features = df1.select(\"features\").rdd.map(lambda row: row[0]).cache()\n",
    "        model = KMeans.train(features, k, maxIterations=40, runs=10, initializationMode=\"random\", seed=20)\n",
    "\n",
    "        #Adding the prediction column to df1\n",
    "        modelBC = sc.broadcast(model)\n",
    "        predictUDF = udf(lambda x: modelBC.value.predict(x), StringType())\n",
    "        df2 = df1.withColumn(\"prediction\", predictUDF(df1.features)).cache()\n",
    "        df2.show()\n",
    "\n",
    "        #Adding the score column to df2; The higher the score, the more likely it is an anomaly \n",
    "        df3 = self.addScore(df2).cache()\n",
    "        df3.show()    \n",
    "\n",
    "        return df3.where(df3.score > t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------------+\n",
      "| id|     rawFeatures|            features|\n",
      "+---+----------------+--------------------+\n",
      "|  0|[http, udt, 0.4]|[0.0, 1.0, 0.0, 1...|\n",
      "|  1|[http, udf, 0.5]|[0.0, 1.0, 0.0, 0...|\n",
      "|  2|[http, tcp, 0.5]|[0.0, 1.0, 1.0, 0...|\n",
      "|  3|[ftp, icmp, 0.1]|[1.0, 0.0, 0.0, 0...|\n",
      "|  4|[http, tcp, 0.4]|[0.0, 1.0, 1.0, 0...|\n",
      "+---+----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muhammadmuhaimin/spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------------+----------+\n",
      "| id|     rawFeatures|            features|prediction|\n",
      "+---+----------------+--------------------+----------+\n",
      "|  0|[http, udt, 0.4]|[0.0, 1.0, 0.0, 1...|         0|\n",
      "|  1|[http, udf, 0.5]|[0.0, 1.0, 0.0, 0...|         0|\n",
      "|  2|[http, tcp, 0.5]|[0.0, 1.0, 1.0, 0...|         1|\n",
      "|  3|[ftp, icmp, 0.1]|[1.0, 0.0, 0.0, 0...|         0|\n",
      "|  4|[http, tcp, 0.4]|[0.0, 1.0, 1.0, 0...|         1|\n",
      "+---+----------------+--------------------+----------+\n",
      "\n",
      "+----------------+--------------------+----------+-----+\n",
      "|     rawFeatures|            features|prediction|score|\n",
      "+----------------+--------------------+----------+-----+\n",
      "|[http, udt, 0.4]|[0.0, 1.0, 0.0, 1...|         0|  0.0|\n",
      "|[http, udf, 0.5]|[0.0, 1.0, 0.0, 0...|         0|  0.0|\n",
      "|[http, tcp, 0.5]|[0.0, 1.0, 1.0, 0...|         1|  1.0|\n",
      "|[ftp, icmp, 0.1]|[1.0, 0.0, 0.0, 0...|         0|  0.0|\n",
      "|[http, tcp, 0.4]|[0.0, 1.0, 1.0, 0...|         1|  1.0|\n",
      "+----------------+--------------------+----------+-----+\n",
      "\n",
      "2\n",
      "+----------------+--------------------+----------+-----+\n",
      "|     rawFeatures|            features|prediction|score|\n",
      "+----------------+--------------------+----------+-----+\n",
      "|[http, tcp, 0.5]|[0.0, 1.0, 1.0, 0...|         1|  1.0|\n",
      "|[http, tcp, 0.4]|[0.0, 1.0, 1.0, 0...|         1|  1.0|\n",
      "+----------------+--------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ad = AnomalyDetection()\n",
    "    use_toy_data = False\n",
    "    ad.readData('./data/logs-features-sample', use_toy_data)\n",
    "    if use_toy_data:\n",
    "        anomalies = ad.detect(2, 0.9)\n",
    "    else:\n",
    "        anomalies = ad.detect(8, 0.97)\n",
    "    print(anomalies.count())\n",
    "    anomalies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
